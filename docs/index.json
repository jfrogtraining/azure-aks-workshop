[
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/",
	"title": "JFrog Platform on Azure Workshop",
	"tags": [],
	"description": "",
	"content": "JFrog Platform on Azure Workshop Welcome In this workshop you will learn about the JFrog Platform and how to leverage Artifactory and Xray for managing your Software Development Lifecycle (SDLC) and bring DevOps to the cloud on Azure.\nLearning Objectives  Understand the roles of Artifactory and Xray in your software delivery life cycle (SDLC). Use Local, Remote and Virtual Repositories in Artifactory. Publish and promote your artifacts and Build Info. Scan your artifacts and builds for security vulnerabilities. Build and deploy your application onto Azure Kubernetes Service (AKS).  The examples and sample code provided in this workshop are intended to be consumed as instructional content. These will help you understand how various services can be architected to build a solution while demonstrating best practices along the way. These examples are not intended for use in production environments.\n "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/1_prerequisites.html",
	"title": "Prerequisites",
	"tags": [],
	"description": "",
	"content": "The following items are required for this workshop.\n  Azure account and environment - If you are at an JFrog Azure event, an account and environment will be provided. Otherwise, go here to create an Azure account.\n  JFrog Platform instance - Use the JFrog Platform Cloud Free Tier to get your own JFrog Platform instance with Artifactory and Xray.\n  A GitHub account and Git installed.\n  Have a notepad available for saving important workshop data.\n  "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/2_devops_cloud.html",
	"title": "DevOps in the Cloud",
	"tags": [],
	"description": "",
	"content": "The goal of DevOps is to allow your development teams to deliver quality software faster to your customers through continuous process improvement, leveraging the best of breed development tools and infrastructure, and utilizing software development and IT operations best practices. Your team must deliver software faster than your competitors in order to get features and fixes to your customers sooner. JFrog terms this ideal as liquid software.\n Looking forward, as release cycles get shorter and microservices get smaller, we can imagine a world in which at any one time, our systems’ software is being updated. Effectively, software will become liquid in that products and services will be connected to “software pipes” that constantly stream updates into our systems and devices; liquid software continuously and automatically updating our systems with no human intervention.\n\u0026ndash; JFrog (2017), A Vision of Liquid Software, Retrieved from https://jfrog.com/whitepaper/a-vision-of-liquid-software/ A critical aspect of DevOps is infrastructure. Cloud computing infrastructure has allowed DevOps to advance and come closer to realizing liquid software. Cloud computing has allowed development teams to build these software pipes by:\n Using ephemeral cloud infrastructure to scale their development process and software delivery at levels not achievable with on-premise infrastructure. Providing applications on a global scale with real-time response and resiliency. Leveraging new cloud services in their application and software development processes to improve the quality, security and delivery of their applications. Allowing multi-discipline teams to collaborate in the cloud across the software lifecycle to ensure quality, security, velocity and scale of applications.  "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/3_workshop.html",
	"title": "Workshop Overview",
	"tags": [],
	"description": "",
	"content": "In this workshop, we will demonstrate DevOps in the cloud with Azure and JFrog. We will build and deploy a containerized NPM application. Using the JFrog Platform and the JFrog CLI, we will compile our code, build our NPM package, execute a docker build and push, security scan the image and publish it to a repository. We will then deploy the image and serve the application with Azure AKS.\n"
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/4_workshop_setup.html",
	"title": "Workshop Setup",
	"tags": [],
	"description": "",
	"content": "Before we get started on building, publishing and deploying our application, we must set up our workshop environment. In this setup section, we will:\n Set up our Azure account and environment. Configure the Azure Cloud Shell. Clone our workshop GitHub repository which contains our code. Prepare our JFrog Platform instance.  "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/5_build_deploy_app.html",
	"title": "Build and Deploy the App",
	"tags": [],
	"description": "",
	"content": "In this section, we will set up our CI/CD pipeline with JFrog Pipelines. Our pipeline, will take our application and build a docker image. Then it will promote it. Finally, it will deploy it to our Azure AKS Cluster. We will:\n Set up our JFrog Pipelines integrations to connect to GitHub, our Artifactory and Azure AKS Kubernetes cluster. Update our CI/CD pipeline. Add our CI/CD pipeline to JFrog Pipelines. Then build and deploy our app.  In this workshop, we use Docker, but the JFrog Platform is a universal solution supporting all major package formats including Alpine, Maven, Gradle, Docker, Conda, Conan, Debian, Go, Helm, Vagrant, YUM, P2, Ivy, NuGet, PHP, NPM, RubyGems, PyPI, Bower, CocoaPods, GitLFS, Opkg, SBT and more.\n "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/6_view_results.html",
	"title": "View Results in JFrog",
	"tags": [],
	"description": "",
	"content": "We have built and published our NPM package and Docker image. Let\u0026rsquo;s view these results in JFrog Artifactory.\n  Go to your JFrog Platform instance and switch to the Packages view in Artifactory. Go to Artifactory ► Packages.\n  Type workshop-app and search. This will show the NPM package that was published with the JFrog CLI.\n  Click on it to view the details.   Go back to the Packages view and search for npm-app. This shows the Docker image that was published.\n  Click on the docker npm-app listing.   This will show a list of the versions. Click on the latest version that was built.   In the Xray Data tab, view the security violations. License violations are available in the JFrog Platform Pro and Enterprise tiers.   Click on any violation to see the details and impact in the Issue Details tab.   Scroll down to the References section to access links to documentation that can help you remediate the issue. In many cases, you just need to update the component and Xray will indicate this.   Xray supports all major package types, understands how to unpack them, and uses recursive scanning to see into all of the underlying layers and dependencies of components, even those packaged in Docker images, and zip files. The comprehensive vulnerability intelligence databases are constantly updated giving the most up-to-date understanding of the security and compliance of your binaries.\n Close the Issue Details tab. View the Docker configuration for the image in the Docker Layers tab. On the Builds tab, click on npm_build in the list.  Then click on your most recent build. In the Published Modules tab, view the set of artifacts and dependencies for your build.   Our JFrog CLI CI/CD \u0026ldquo;pipeline\u0026rdquo; provided an overview of a typical build, docker build and push, security scan and promotion process using Artifactory and Xray.\nNext, we will deploy your docker image from the \u0026ldquo;staging\u0026rdquo; repository using Azure Kubernetes Service (AKS).\n"
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/7_make_changes.html",
	"title": "Make Changes",
	"tags": [],
	"description": "",
	"content": "We have built and published our NPM package and Docker image. Let\u0026rsquo;s view these results in JFrog Artifactory.\n  Go to your JFrog Platform instance and switch to the Packages view in Artifactory. Go to Artifactory ► Packages.\n  Type workshop-app and search. This will show the NPM package that was published with the JFrog CLI.\n  Click on it to view the details.   Go back to the Packages view and search for npm-app. This shows the Docker image that was published.\n  Click on the docker npm-app listing.   This will show a list of the versions. Click on the latest version that was built.   In the Xray Data tab, view the security violations. License violations are available in the JFrog Platform Pro and Enterprise tiers.   Click on any violation to see the details and impact in the Issue Details tab.   Scroll down to the References section to access links to documentation that can help you remediate the issue. In many cases, you just need to update the component and Xray will indicate this.   Xray supports all major package types, understands how to unpack them, and uses recursive scanning to see into all of the underlying layers and dependencies of components, even those packaged in Docker images, and zip files. The comprehensive vulnerability intelligence databases are constantly updated giving the most up-to-date understanding of the security and compliance of your binaries.\n Close the Issue Details tab. View the Docker configuration for the image in the Docker Layers tab. On the Builds tab, click on npm_build in the list.  Then click on your most recent build. In the Published Modules tab, view the set of artifacts and dependencies for your build.   Our JFrog CLI CI/CD \u0026ldquo;pipeline\u0026rdquo; provided an overview of a typical build, docker build and push, security scan and promotion process using Artifactory and Xray.\nNext, we will deploy your docker image from the \u0026ldquo;staging\u0026rdquo; repository using Azure Kubernetes Service (AKS).\n"
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/8_conclusion.html",
	"title": "Conclusion",
	"tags": [],
	"description": "",
	"content": "In this workshop, we used the JFrog Platform to build an application, manage the artifacts, scan the artifacts for security vulnerabilities and license compliance, and publish the artifacts of your application to a staging repository. Then we used Azure AKS to deploy your application so that end-users can access it. The JFrog Platform and Azure AKS to demonstrate how you can build a DevOps cloud platform on Azure to delivery your software to your end-users. This modernizes your software delivery life cycle enabling your organization to deliver quality software continuously by leveraging advanced cloud services and elastic infrastructure.\n"
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/2_devops_cloud/21_continuous_integration_and_delivery.html",
	"title": "Continuous Integration and Delivery",
	"tags": [],
	"description": "",
	"content": "Continuous integration and delivery (CI/CD) is the process for which your software components are built from code, integrated, tested, released, deployed and ultimately delivered to end-users. CI/CD pipelines are the software assembly line that orchestrates the building of your software. This CI/CD pipeline line requires infrastructure. Cloud computing has allowed this infrastructure to become dynamic and ephemeral. On cloud infrastructure, your CI/CD pipelines scale up and down to meet your software delivery demands. It saves costs by providing the right amount of cloud infrastructure just as it is needed. This is further realized by using cloud-native technologies like Kubernetes and extending across clouds and on-premise datacenters. The following are some Azure cloud technologies that CI/CD pipelines can utilize:\n Azure Virtual Machines can be used as CI/CD pipeline nodes that can be dynamically spun up and down to execute pipeline tasks. Azure Spot Virtual Machines can dramatically lower costs by utilizing spare capacity nodes for CI/CD pipeline tasks. Azure Kubernetes Service (AKS) can provide a Kubernetes-based CI/CD worker node pools and allow more efficient use of compute resources. Azure Stack can allow you to span your CI/CD pipelines from your on-premise datacenter to the cloud for hybrid and migration use cases.  "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/2_devops_cloud/22_binary_repository_management.html",
	"title": "Binary Repository Management",
	"tags": [],
	"description": "",
	"content": "A Binary Repository Manager is a software hub that simplifies the development process for different teams across an organization by helping them to collaborate on building coherent and compatible software components. It does this by centralizing the management of all the binary artifacts generated and used by the organization, thereby overcoming the incredible complexity arising from diverse binary artifact types, their position in the overall workflow and the set of dependencies between them.\nSome of the many benefits of using a Binary Repository Manager are:\n Reliable and consistent access to remote artifacts. Reduced network traffic and optimized builds. Tight integration with build ecosystems. Custom handling of artifacts to comply with any organization’s requirements. Security and access control to artifacts and repositories. Manage licensing requirements and open source governance for use of software components. Distributing and sharing artifacts across an organization. System stability and reliability with high availability architecture. Smart search for binaries. Advanced maintenance and monitoring tools.  Cloud infrastructure has provided additional benefits. With the cloud, binary repositories can now:\n Enable replication and resiliency through the use of global data centers. Provide lower latency and improved network performance by being available closer to end-users. Provide their services at the edge of the network regionally and globally to edge devices. Utilize cloud storage for reduced costs, scalability and lower maintenance. Leverage cloud services such as security vulnerability databases to extend their functionality.  "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/2_devops_cloud/23_dev_sec_ops.html",
	"title": "DevSecOps",
	"tags": [],
	"description": "",
	"content": "Any security issue identified by a security scanning may be reviewed by a small security team that may lack the technical knowledge. This challenge can be reduced by shifting left to the developer and operations teams, making them also responsible for security and compliance. This moves security earlier in the software delivery process. Source code, dependency and artifact security scanning are some examples of moving security into the development process. Implementing the identification of security issues earlier in the CI/CD pipeline, as well as automating security and compliance policies in the Software Development Lifecycle (SDLC), rather than using manual processes, is crucial. Moreover, organizations that leave the Sec out of DevOps, may face security and compliance issues that are closer to their release, resulting in additional costs for remediating such issues.\nAs you move your SDLC to the cloud, your DevSecOps strategy must also adapt to the cloud. As discussed previously, binary repository managers that scale globally across cloud data centers require DevSecOps tools that will likewise scale and adjust. An enterprise scale software delivery system with multiple development teams, end users and devices mean more entry points for potential security and compliance issues. Therefore, it is critical that your SLDC is well-integrated with your DevSecOps system.\n"
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/2_devops_cloud/24_jfrog_platform_overview.html",
	"title": "JFrog Platform for DevOps in the Cloud",
	"tags": [],
	"description": "",
	"content": "The JFrog Platform is designed to meet the growing needs of companies to develop and distribute software in the cloud. It provides DevOps teams with the tools needed to create, manage, secure and deploy software with ease. These tools cover everything from continuous integration and delivery (CI/CD), binary repository management, artifact maturity, security and vulnerability protection (DevSecOps), release management, analytics and distribution.\nJFrog Artifactory is an Artifact Repository Manager that fully supports software packages created by any language or technology. Furthermore, it integrates with all major CI/CD and DevOps tools to provide an end-to-end, automated solution for tracking artifacts from development to production.\nJFrog Xray provides universal artifact analysis, increasing visibility and performance of your software components by recursively scanning all layers of your organization’s binary packages to provide radical transparency and unparalleled insight into your software architecture.\nJFrog Distribution empowers DevOps to distribute and continuously update remote locations with release-ready binaries.\nJFrog Artifactory Edge accelerates and provides control of release-ready binary distribution through a secure distributed network and edge nodes.\nJFrog Mission Control and Insight is your DevOps dashboard solution for managing multiple services of Artifactory, Xray, Edge and Distribution.\nJFrog Access with Federation provides governance to the distribution of artifacts by managing releases, permissions and access levels.\nJFrog Pipelines helps automate the non-human part of the whole software development process with continuous integration and empowers teams to implement the technical aspects of continuous delivery.\nAll of these JFrog Platform components are designed and developed to work together out-of-the-box with minimal configuration. Management and monitoring of your software delivery lifecycle from build to distribution is accessible though a central, unified user interface. The JFrog platform is enterprise ready with your choice of on-prem, cloud, multi-cloud or hybrid deployments that scale as you grow.\n "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/4_workshop_setup/41_azure_setup.html",
	"title": "Azure Setup",
	"tags": [],
	"description": "",
	"content": "In this section, we will setup our Azure environment. We will:\n Get an Azure environment. Configure the Azure Cloud Shell. Create the AKS cluster. Get the workshop code.  Please choose if you are running the workshop on your own or attending a JFrog Azure event.\n \u0026hellip;running the workshop on your own, or \u0026hellip;attending a JFrog Azure hosted event  "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/4_workshop_setup/42_jfrog_setup.html",
	"title": "JFrog Platform Setup",
	"tags": [],
	"description": "",
	"content": "Next, we will setup our JFrog Platform instance. We will:\n Get a free JFrog Platform instance. Activate JFrog Pipelines CI/CD Set up our docker repositories. Configure Xray to scan our repositories for security vulnerabilities.  "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/5_build_deploy_app/51_pipeline_integrations.html",
	"title": "Set up our JFrog Pipelines Integrations",
	"tags": [],
	"description": "",
	"content": "Our CI/CD pipeline requires access to GitHub to pull our code, access to JFrog Artifactory to publish our Docker image and build info and access to the AKS cluster to deploy our application. We will set up JFrog Pipelines integrations to enable these.\nAn Integration connects Pipelines to an external service/tool. Each integration type defines the endpoint, credentials and any other configuration detail required for Pipelines to exchange information with the service. All credential information is encrypted and held in secure storage, in conformance with best security practices.\n   In your JFrog Platform instance, go to Administration \u0026gt; Pipelines \u0026gt; Integrations.\n  Click Add an Integration.\n  For the Name, enter github_integration.\n  For Integration Type, select GitHub.\n  Copy and paste your GitHub personal access token.\nEnsure it has these minimum GitHub permissions:\n repo (all) admin:repo_hook (read, write) admin:public_key (read, write)    Click Test connection to validate.\n  Click Create to create the integration.\n   Click Add an Integration again.\n  For the Name, enter artifactory_integration.\n  For Integration Type, select Artifactory.\n  Click Get API Key to generate an API key.\n  Click Test connection to validate.\n  Click Create to create the integration.\n   Click Add an Integration again.\n  For the Name, enter aks_integration.\n  For Integration Type, select Kubernetes.\n  Copy and paste your AKS Kubernetes cluster kubeconfig.\n  Click Create to create the integration.\n  Congratulations! We have created the integrations that are required for our CI/CD pipeline.\n"
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/5_build_deploy_app/52_update_pipeline.html",
	"title": "Update our Pipeline",
	"tags": [],
	"description": "",
	"content": "We need to make an update to our CI/CD pipeline in order to pull code from your GitHub repository and to use your personal JFrog Artifactory instance. The CI/CD pipeline is defined in pipelines.yml. This pipeline file is parameterized with a values.yml file. We need to update this file.\n In your Azure Cloud Shell, use the editor and view the pipelines.yml file. View and understand the steps. Note the parameterized values.  In the editor, select the values.yml file and updated the parameterized values to point to your GitHub repository and JFrog Artifactory instance. Use CTRL+S or CMD+S to save the file.  In your Azure Cloud Shell, change directory to azure-aks-workshop. Commit these changes.  cd ~/azure-aks-workshop git add . git commit -m 'Updated values.yml.' Next, push these updates. When prompted for a username and password, use your GitHub username and personal access token.  git push origin master\nWe are now ready to add your CI/CD pipeline and execute!\n"
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/5_build_deploy_app/53_add_pipeline.html",
	"title": "Add our CI/CD Pipeline",
	"tags": [],
	"description": "",
	"content": " In your JFrog Platform instance, go to Application \u0026gt; Pipelines \u0026gt; Pipeline Sources.   Click Add a pipeline source.\n  Click Add Pipeline Source at the top right and select From YAML.\n   For SCM Provider Integration, select the github_integration that you created previously.\n  For Repository Full Name, select your forked /azure-aks-workshop.\n  For Branch, select master.\n  Leave Pipeline Config File Filter as pipelines.yml.\n  Click Create Source. JFrog Pipelines will process the CI/CD pipeline. The status should show Not Synced then Syncing and then Success.  Go to Application \u0026gt; Pipelines \u0026gt; My Pipelines. Notice that your pipeline has a status of Not Built.  Click on your pipeline, workshop_app_build.  Click on the docker_build step and trigger the step to execute the pipeline. JFrog Pipelines will allocate build nodes and execute your pipeline.  It will take a few minutes to execute. Click on the Run to monitor the status of each step.  Use the pulldown to select each step and view the logs.  "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/4_workshop_setup/41_azure_setup/412_azure_event_account.html",
	"title": "JFrog Azure Event: Get an Azure Environment",
	"tags": [],
	"description": "",
	"content": " Only complete this section if you are running the workshop through an JFrog Azure hosted event. If you are not at a JFrog Azure hosted event, go here.\n For an JFrog Azure hosted event, you will be provided with a registration link and an activation code. The following steps show how to use the activation code to access an Azure environment for this workshop.\n Go to the registration link in your browser. Fill out the details in the registration form and enter the activation code.  Then click Launch Lab on the next screen.  Wait a few moments for your Azure environment to be created. When ready, you will be presented with your Azure environment credentials.  Copy these credentials. Use these Azure Credentials to login to the Azure Portal here. Once logged in, you will be directed to the Azure Portal landing page.   Your workshop Azure environment expires after 48 hours.\n "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/4_workshop_setup/41_azure_setup/413_self_paced_account.html",
	"title": "Self-paced: Create an Azure account",
	"tags": [],
	"description": "",
	"content": " Only complete this section if you are running the workshop on your own. If you are at an JFrog Azure hosted event, go here.\n   If you don\u0026rsquo;t already have an Azure account, create one now by going here.\n  Select Start free.\n  Sign in with your Microsoft or GitHub account or create a free Microsoft account.\n  On the About you page, select your correct country or region. Enter your first and last name, email address, and phone number. Depending on your country, you might see additional fields, such as a VAT number. Select Next to continue.\n  On the Identity verification by phone screen, select your country code, and type the number of a telephone to which you have immediate access.\n  You have the option of text or callback to obtain a verification code. Select the relevant button, type the code in the Verification code box, and select Verify code.\n  If the verification code is correct, you\u0026rsquo;re asked to enter details of a valid credit card. Enter the card information and select Next.\n  The last step is to review the agreement and privacy statement then select Sign up.\n  Congratulations! You have successfully set up a free account, and you will be redirected to the Azure portal home page.\n"
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/4_workshop_setup/41_azure_setup/414_azure_cloud_shell.html",
	"title": "Set up your Azure Cloud Shell",
	"tags": [],
	"description": "",
	"content": "Azure Cloud Shell is an interactive, authenticated, browser-accessible shell for managing Azure resources.\n In your Azure Portal, click the Azure Cloud Shell button.  You will be prompted for a shell type. Choose Bash.  If this is your first time using Azure Cloud Shell, you will be prompted to create storage to support it. Go ahead and do this by clicking Create storage with the provided subscription. It will take a few minutes to set up the storage.   Click on Show advanced settings.\n  Use the existing jfrog-azure-resource-xxx group. Specify new unique values for storage and file share.\n  Click Create storage. Wait a few moments for the Azure Cloud Shell to be set up.  Execute the following command to list your Azure Resource groups.  az group list\n Note the resource group named jfrog-azure-workshop-xxxx and the region/location. Copy these values. The resources that we create in this workshop will be created in this resource group and region.\n  Set environment variables for your resource group and region.\n  export REGION=\u0026lt;region/location\u0026gt;\nexport RESOURCE_GROUP=\u0026lt;resource group\u0026gt;\nA resource group is a container that holds related resources for an Azure solution. The resource group can include all the resources for the solution. Generally, add resources that share the same lifecycle to the same resource group so that you can easily deploy, update, and delete them as a group.\n "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/4_workshop_setup/41_azure_setup/415_azure_service_principal.html",
	"title": "Create a Service Principal",
	"tags": [],
	"description": "",
	"content": "To deploy our application with our CI/CD pipeline, we need Azure credentials. To do this, we will create an Azure service principal.\nWhen you have applications, hosted services, or automated tools that needs to access or modify resources, you can create an identity for the app. This identity is known as a service principal. Access to resources is restricted by the roles assigned to the service principal, giving you control over which resources can be accessed and at which level. For security reasons, it\u0026rsquo;s always recommended to use service principals with automated tools rather than allowing them to log in with a user identity.\n  In your Azure Cloud Shell, let\u0026rsquo;s create an Azure Service Principal using the CLI.  az ad sp create-for-rbac "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/4_workshop_setup/41_azure_setup/416_create_cluster.html",
	"title": "Create an AKS Cluster",
	"tags": [],
	"description": "",
	"content": "Before creating an AKS cluster for our application, we must first set up the required networking with the following steps.\n Now we are ready to create an AKS cluster. In your Azure Cloud Shell, let\u0026rsquo;s get the latest version of Kubernetes that is available with the following command.  VERSION=$(az aks get-versions \\ --location $REGION \\ --query 'orchestrators[?!isPreview] | [-1].orchestratorVersion' \\ --output tsv) echo $VERSION Let\u0026rsquo;s create an environment variable for the cluster name.  CLUSTER_NAME=jfrog-azure-workshop-cluster echo $CLUSTER_NAME Now we are ready to create the AKS cluster. Execute the following command. This will take a few minutes. We can move onto the next step while we wait.  az aks create \\ --resource-group $RESOURCE_GROUP \\ --name $CLUSTER_NAME \\ --vm-set-type VirtualMachineScaleSets \\ --node-count 2 \\ --load-balancer-sku standard \\ --location $REGION \\ --kubernetes-version $VERSION \\ --network-plugin azure \\ --generate-ssh-keys \\ --enable-managed-identity    What is this command doing?   .\n  Next, let\u0026rsquo;s update our kubeconfig to get access to his cluster.  az aks get-credentials --resource-group $RESOURCE_GROUP --name $CLUSTER_NAME\nA kubeconfig file is a file used to configure access to Kubernetes clusters when used in conjunction with the kubectl commandline tool (or other clients).\n Finally, execute the following command and copy the kubeconfig so that we can use it to access the cluster from our CI/CD pipeline.  cat ~/.kube/config "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/4_workshop_setup/41_azure_setup/417_workshop_code.html",
	"title": "Get the Workshop Code",
	"tags": [],
	"description": "",
	"content": "The workshop code is located at https://github.com/jfrogtraining/azure-aks-workshop GitHub repository. We will fork and clone this repository in order to pull the required workshop files and scripts.\n Go to https://github.com/jfrogtraining/azure-aks-workshop and fork this repository to your GitHub account.  Clone the repo. You can do this locally in the Azure Cloud Shell. You can open another Azure Cloud Shell session.  git clone https://github.com/\u0026lt;your user name\u0026gt;/azure-aks-workshop.git\nSet your GitHub username and email.  git config --global user.name \u0026lt;GitHub username\u0026gt;\ngit config --global user.email \u0026lt;GitHub email\u0026gt;\nYou can use the Azure Cloud Shell editor to explore the code.   We will also need a GitHub personal access token to make updates and for JFrog Pipelines CI/CD. You may already have one. If not, follow these instructions to create one. Copy it to your notepad.\nEnsure it has these minimum GitHub permissions:\n repo (all) admin:repo_hook (read, write) admin:public_key (read, write)    "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/4_workshop_setup/42_jfrog_setup/421_jfrog_free.html",
	"title": "Get a Free JFrog Platform Instance",
	"tags": [],
	"description": "",
	"content": "If you do not have access to a JFrog Platform instance, use the JFrog Platform Cloud Free Tier to get your own JFrog Platform instance with Artifactory, Xray and Pipelines.\nWhen signing up for the JFrog Platform Cloud Free Tier, ensure that you select Azure and the region for your resource group.\n  JFrog Platform Cloud Free Tier   "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/4_workshop_setup/42_jfrog_setup/422_jfrog_pipelines.html",
	"title": "Activate JFrog Pipelines CI/CD",
	"tags": [],
	"description": "",
	"content": "  Go to your JFrog Platform instance at https://[server name].jfrog.io. Refer to your JFrog Free Subscription Activation email if needed. Substitute your server name.   Login to your JFrog Platform instance with your credentials.   Once logged into your JFrog Platform instance, you will be presented with the landing page.   Go to Application \u0026gt; Pipelines and follow the instructions to activate JFrog Pipelines. This will take a few minutes.\n  Move onto the next step while JFrog Pipelines activates.\n"
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/4_workshop_setup/42_jfrog_setup/423_docker_repositories.html",
	"title": "Set Up Docker Repositories",
	"tags": [],
	"description": "",
	"content": " In your JFrog Platform instance at the top right, enable the drop down menu and select Quick Setup.  On the Create Repositories dialog, choose Docker and click Next.   Next, enter workshop for the Repositories Prefix.\n  Click Create. This will create the following docker repositories:\n   workshop-docker-local workshop-docker-remote workshop-docker   Local repositories are physical, locally-managed repositories into which you can deploy artifacts. These are repositories that are local to the JFrog Artifactory instance. A remote repository serves as a caching proxy for a repository managed at a remote URL (which may itself be another Artifactory remote repository). A virtual repository (or \u0026ldquo;repository group\u0026rdquo;) aggregates several repositories with the same package type under a common URL. A virtual repository can aggregate local and remote repositories.  Next, let\u0026rsquo;s create another docker repository to represent production images. Click the Add Repositories button and select Local Repository.   Select Docker for the Package Type.\n  Name this docker repository workshop-docker-prod-local. Click Save \u0026amp; Finish.\n  Click on the Virtual tab under Repositories.   Click on the workshop-docker virtual repository.\n  Move the new workshop-docker-prod-local repository under Selected Repositories.\n  Under Default Deployment Repository, select workshop-docker-prod-local as the default deployment repository. This means that image deployments from virtual repository workshop-docker will actually deploy from the local repository workshop-docker-prod-local.   Click Save \u0026amp; Finish.\n  Under the Local tab under Repositories, select the workshop-docker-local repository.\n   Scroll down and check Enable Indexing in Xray. This tells Xray to automatically scan this repository.\n  Click Save \u0026amp; Finish.\n  Go to Administration \u0026gt; Identity and Access \u0026gt; Permissions.\n  Click the Anything permission. This sets the permissions for all of the repositories including the new Docker repositories that we just created.   Click on the Users tab.\n  Check Deploy/Cache. For our workshop, this will allow us to deploy our Docker image to our AKS cluster without authenticating.\n  Click Save.  Congratulations! You have set up your Docker repositories. Now let\u0026rsquo;s configure our security policies for Xray.\n"
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/4_workshop_setup/42_jfrog_setup/424_xray_setup.html",
	"title": "Set Up Xray Security",
	"tags": [],
	"description": "",
	"content": " In your JFrog Platform instance, go to Administration \u0026gt; Xray \u0026gt; Watches \u0026amp; Policies.   Click Create a Policy.\n  Call the security policy, High-Severity.\n   Click on New Rule.\n  Name the rule High-Severity and select High for the Minimal Severity. Click Save.\n   Click Create to create this new security policy.\n  Click on the Watches tab under Watches \u0026amp; Policies.\n   Click on Set up a Watch.\n  Name the new watch Docker-Scanning.\n  Click Add Repositories.\n  Move the repositories workshop-docker-local and workshop-docker-prod-local to the Included Repositories.\n   Click Save.\n  Click Manage Policies.\n  Move the High-Severity policy to the Included Policy.\n   Click Save.\n  Click Create to create the new watch. This watch will scan the workshop-docker-local and workshop-docker-prod-local Docker repositories for new images and check for high severity security vulnerabilities.\n  JFrog Xray scans your artifacts, builds and release bundles for OSS components, and detects security vulnerabilities and licenses in your software components. Policies and Watches allow you to enforce your organization governance standards. Setup up your Policies and Watches to reflect standard governance behaviour specifications for your organization across your software components.\n "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/categories.html",
	"title": "Categories",
	"tags": [],
	"description": "",
	"content": ""
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/cleanup.html",
	"title": "Cleanup",
	"tags": [],
	"description": "",
	"content": "  Your JFrog Platform Instance - The JFrog Platform instance that you used in this workshop will automatically be destroyed after the workshop. There isn\u0026rsquo;t anything you need to do. If you would like keep it, you can upgrade to one of the premium plans. Do this by clicking on the Upgrade button.   Azure Resources\n View your Azure Resource Groups here. Click on the jfrog-azure-workshop resource group. Click on Delete resource group.  Confirm deletion and click Delete. Your workshop resources will be deleted after a few minutes.     "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/resources.html",
	"title": "Resources",
	"tags": [],
	"description": "",
	"content": " JFrog Platform Documentation - The full documentation of the JFrog Platform, the universal, hybrid, end-to-end DevOps automation solution. It is designed to take you through all the JFrog Products. Including user, administration and developer guides, installation and upgrade procedures, system architecture and configuration, and working with the JFrog application. JFrog Academy - Learn more about the JFrog Platform at your own pace with JFrog Academy free courses taught by our experts.  "
},
{
	"uri": "https://jfrogtraining.github.io/azure-aks-workshop/tags.html",
	"title": "Tags",
	"tags": [],
	"description": "",
	"content": ""
}]